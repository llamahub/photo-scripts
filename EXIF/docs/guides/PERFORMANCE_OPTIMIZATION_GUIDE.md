# Performance Optimization Guide for Image Analysis

## ðŸš€ Speed Optimization Strategies

When analyzing folders with many images (thousands to millions), here are the key optimization strategies:

## 1. Use the Optimized Analyzer

### Quick Start
```bash
# Maximum speed - analyze without target path generation
python scripts/analyze_fast.py --source /path/to/images

# Include target comparison (slower but more comprehensive)
python scripts/analyze_fast.py --source /path/to/images --target /path/to/target

# Analyze a sample first for quick insights
python scripts/analyze_fast.py --source /path/to/images --sample 1000

# Tune performance parameters
python scripts/analyze_fast.py --source /path/to/images --workers 16 --batch-size 200
```

### Performance Improvements
- **10-50x faster** than original analyzer
- **Batch ExifTool calls** - Process 100+ files per ExifTool invocation instead of 1
- **Parallel processing** - Use all CPU cores simultaneously  
- **Optimized file discovery** - Fast pathlib-based file finding
- **Progress reporting** - Real-time progress updates

## 2. Key Performance Parameters

### `--target` (Optional)
- **Omit for maximum speed**: Skip slow target path generation
- **Include for comparison**: Generate target paths and check existence
- **Speed difference**: 2-5x faster without target processing

```bash
# Maximum speed - no target processing
python scripts/analyze_fast.py --source /path/to/images

# With target comparison (slower but comprehensive)
python scripts/analyze_fast.py --source /path/to/images --target /organized
```

### `--batch-size` (Default: 100)
- **Larger batches = fewer subprocess calls**
- Recommended: 50-200 for most systems
- Larger values use more memory but are faster
- Too large may hit command-line length limits

```bash
# For fast SSDs and lots of RAM
python scripts/analyze_fast.py --source /path --batch-size 200

# For slower storage or limited memory  
python scripts/analyze_fast.py --source /path --batch-size 50
```

### `--workers` (Default: auto-detect)
- **More workers = more parallelism**
- Default: `min(32, CPU_count + 4)`
- Optimal: Usually 2-4x your CPU core count
- Too many workers can overwhelm I/O

```bash
# For 8-core CPU
python scripts/analyze_fast.py --source /path --target /path --workers 16

# For network storage (reduce I/O pressure)
python scripts/analyze_fast.py --source /path --target /path --workers 4
```

### `--sample` (Analysis Preview)
- **Analyze subset for quick insights**
- Perfect for initial assessment of large collections
- Random sampling gives representative results

```bash
# Quick overview of 1000 random images
python scripts/analyze_fast.py --source /huge/collection --target /target --sample 1000
```

## 3. Hardware Optimizations

### Storage
- **SSD vs HDD**: SSDs provide 5-10x faster file access
- **Local vs Network**: Local storage much faster than network drives
- **RAID configurations**: RAID 0 or NVMe can significantly improve I/O

### CPU
- **Multi-core systems**: More cores = better parallel performance
- **ExifTool is CPU-bound**: Faster CPUs process metadata quicker

### Memory
- **Larger batches need more RAM**: 8GB+ recommended for large batches
- **Each worker uses memory**: More workers = more RAM usage

## 4. Analysis Strategies for Different Scales

### Small Collections (< 1,000 images)
```bash
# Maximum speed analysis
python scripts/analyze_fast.py --source /photos
```

### Medium Collections (1,000 - 50,000 images)
```bash
# Fast analysis without target paths
python scripts/analyze_fast.py --source /photos

# With target comparison if needed
python scripts/analyze_fast.py --source /photos --target /organized

# Sample first for quick overview
python scripts/analyze_fast.py --source /photos --sample 500
```

### Large Collections (50,000 - 500,000 images)
```bash
# Maximum speed - no target processing
python scripts/analyze_fast.py --source /photos --workers 16 --batch-size 150

# With target comparison (slower)
python scripts/analyze_fast.py --source /photos --target /organized \
  --workers 16 --batch-size 150

# Process in segments if needed
python scripts/analyze_fast.py --source /photos/2020
python scripts/analyze_fast.py --source /photos/2021
```

### Massive Collections (500,000+ images)
```bash
# Ultimate speed - no target processing
python scripts/analyze_fast.py --source /photos --workers 32 --batch-size 200

# Consider folder-by-folder analysis
for year in 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024; do
  python scripts/analyze_fast.py --source /photos/$year \
    --output "analysis_$year.csv" --workers 32 --batch-size 200
done
```

## 5. System-Level Optimizations

### Linux/macOS
```bash
# Increase file descriptor limits
ulimit -n 4096

# Use ionice for background processing
ionice -c3 python scripts/analyze_fast.py --source /photos --target /organized

# Monitor system resources
htop  # Watch CPU and memory usage
iotop # Watch disk I/O
```

### ExifTool Optimization
```bash
# Ensure ExifTool is installed and updated
exiftool -ver  # Should be version 12.0 or higher

# For very large collections, consider ExifTool config optimizations
export EXIFTOOL_CONFIG=/path/to/custom.config
```

## 6. Performance Monitoring

### Built-in Progress Reporting
The optimized analyzer provides real-time progress:
```
Found 15432 images to analyze...
Processed 100/15432 images...
Processed 200/15432 images...
Progress: 1000/15432 (6.5%)
```

### Custom Progress Callback
```python
from exif import OptimizedImageAnalyzer

def my_progress(current, total):
    percent = (current / total) * 100
    print(f"Custom progress: {percent:.1f}% ({current}/{total})")

analyzer = OptimizedImageAnalyzer()
results = analyzer.analyze_images_fast(progress_callback=my_progress)
```

## 7. Expected Performance

### Benchmarks (approximate)
| Collection Size | Original Time | Optimized Time | Speedup |
|----------------|---------------|----------------|---------|
| 1,000 images   | 5 minutes     | 30 seconds     | 10x     |
| 10,000 images  | 50 minutes    | 3 minutes      | 17x     |
| 100,000 images| 8+ hours      | 20 minutes     | 24x     |

### Factors Affecting Speed
- **Storage speed**: SSD vs HDD makes huge difference
- **CPU cores**: More cores = better parallelism
- **Image formats**: RAW files slower than JPEG
- **Network vs local**: Network storage adds significant overhead
- **ExifTool complexity**: More metadata fields = slower processing

## 8. Memory Usage

### Typical Memory Usage
- **Base**: ~50MB for the Python process
- **Per worker**: ~10-20MB additional
- **Per batch**: ~1-5MB per 100 images
- **Large collections**: Plan for 500MB - 2GB peak usage

### Memory-Constrained Systems
```bash
# Reduce batch size and workers for low-memory systems
python scripts/analyze_fast.py --source /photos --target /organized \
  --workers 4 --batch-size 25
```

## 9. Troubleshooting Performance Issues

### Slow Performance
1. **Check storage speed**: `hdparm -t /dev/sdX` (Linux)
2. **Monitor CPU usage**: Should be near 100% during analysis
3. **Check memory**: Ensure no swapping occurs
4. **Reduce batch size**: If hitting memory limits
5. **Network storage**: Consider local temporary copy

### Error Handling
The optimized analyzer handles errors gracefully:
- Individual file failures don't stop processing
- Batch failures fall back to individual processing
- All errors logged in CSV output

### Disk Space
- **CSV output**: ~1KB per 100 images analyzed
- **Log files**: Additional space for progress logs
- **Temporary files**: ExifTool may create temp files

## 10. Integration Examples

### Programmatic Usage
```python
from exif import OptimizedImageAnalyzer

# Create analyzer with custom settings
analyzer = OptimizedImageAnalyzer(
    folder_path="/path/to/images",
    max_workers=16,
    batch_size=150
)

# Fast analysis with progress
results = analyzer.analyze_with_progress()

# Save results
analyzer.save_to_csv("analysis_results.csv")

# Get statistics
stats = analyzer.get_statistics()
print(f"Processed {stats['total_images']} images")
```

### Shell Integration
```bash
#!/bin/bash
# Batch process multiple folders
for folder in /photos/*/; do
  echo "Processing $folder..."
  python scripts/analyze_fast.py \
    --source "$folder" \
    --target "/organized" \
    --output "analysis_$(basename "$folder").csv" \
    --workers 16
done
```

This optimized approach can reduce analysis time from hours to minutes for large photo collections while providing the same detailed analysis results.